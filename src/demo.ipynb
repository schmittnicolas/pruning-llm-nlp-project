{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/miniconda3/envs/nlpa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model_config import ModelConfig\n",
    "from pruning_methods.wanda import wanda_pruning\n",
    "from pruning_methods.magnitude import magnitude_pruning\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 'facebook/opt-350m' from cache directory '.cache/llm_weights/'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from evaluation_pruning import generate_text\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "llama_model = \"meta-llama/Llama-3.2-1B\"\n",
    "modelConfig = ModelConfig(token=token)\n",
    "model = modelConfig.load_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitude Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count the total number of non-zero parameters in a model.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to count parameters for\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (total non-zero parameters, trainable non-zero parameters)\n",
    "    \"\"\"\n",
    "    total_nonzero_params = 0\n",
    "    trainable_nonzero_params = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        num_nonzero_params = torch.count_nonzero(param).item()  # Count non-zero elements\n",
    "        total_nonzero_params += num_nonzero_params\n",
    "        if param.requires_grad:\n",
    "            trainable_nonzero_params += num_nonzero_params\n",
    "    \n",
    "    return total_nonzero_params, trainable_nonzero_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in original model: (331195120, 331195120)\n",
      "number of parameters in prunned model: (166761506, 166761506)\n"
     ]
    }
   ],
   "source": [
    "original_model = modelConfig.model\n",
    "\n",
    "prunned_model = modelConfig.copy_model()\n",
    "\n",
    "pruning_result = magnitude_pruning(prunned_model, 0.5)\n",
    "\n",
    "print(f\"number of parameters in original model: {count_parameters(original_model)}\")\n",
    "print(f\"number of parameters in prunned model: {count_parameters(prunned_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat is a big, and he is in the, he, the one, is the only, no one. no no, but, just no he.\n"
     ]
    }
   ],
   "source": [
    "from evaluation_pruning import generate_text\n",
    "\n",
    "print(generate_text(prunned_model, modelConfig.tokenizer, \"the cat is\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wikitext Perplexity: 100%|██████████| 20/20 [01:26<00:00,  4.34s/it]\n",
      "Wikitext Perplexity: 100%|██████████| 20/20 [01:26<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Perplexity:  23.599618911743164\n",
      "Pruned Model Perplexity:  1771.341064453125\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OPTForCausalLM' object has no attribute 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m original_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m prunned_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mglobal_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprunned_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Prog/pruning-llm-nlp-project/src/evaluation_pruning.py:279\u001b[0m, in \u001b[0;36mglobal_evaluation\u001b[0;34m(modelConfig, original_model, pruned_model, tokenizer, device)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPruned Model Perplexity: \u001b[39m\u001b[38;5;124m\"\u001b[39m, pruned_model_perplexity)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# compare_model_memory(original_model, pruned_model)\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# compare_inference_time(\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m#     original_model,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m#     tokenizer,\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m \u001b[43mcompare_models_for_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpruned_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Prog/pruning-llm-nlp-project/src/evaluation_pruning.py:247\u001b[0m, in \u001b[0;36mcompare_models_for_prompt\u001b[0;34m(original_model, pruned_model, prompt)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompare_models_for_prompt\u001b[39m(original_model, pruned_model, prompt\u001b[38;5;241m=\u001b[39mPROMPT):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Generate output from the original model\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# original_output = original_model(prompt, max_length=100, num_return_sequences=1)[0][\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m#     \"generated_text\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     original_output \u001b[38;5;241m=\u001b[39m generate_text(original_model, \u001b[43moriginal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m, prompt)\n\u001b[1;32m    248\u001b[0m     pruned_output \u001b[38;5;241m=\u001b[39m generate_text(pruned_model, pruned_model\u001b[38;5;241m.\u001b[39mtokenizer, prompt)\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Display the results\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlpa/lib/python3.11/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OPTForCausalLM' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "source": [
    "from evaluation_pruning import global_evaluation\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "original_model.to(device)\n",
    "prunned_model.to(device)\n",
    "\n",
    "global_evaluation(modelConfig, original_model, prunned_model, modelConfig.tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wanda Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wanda_pruning(modelConfig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
