{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "# from evaluation_pruning import PPLMetric\n",
    "import copy\n",
    "from model_config import ModelConfig\n",
    "\n",
    "from evaluation_pruning import global_evaluation\n",
    "from data_loading import get_wikitext2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_config = ModelConfig(model_name='meta-llama/Llama-3.2-1B')\n",
    "\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me a storie. Once upon a time, there was a little\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.2-1B'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "original_param_count = count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Tell me a storie. Once upon a time, there was a little girl who loved to eat. Her mother used to say, “Eat a lot of fish, sweet potatoes, and oranges. They are good for your bones.” This was her secret.\n",
      "One day, the little girl’s mother was not home, and the little girl was very hungry. She decided to take the little girl’s bicycle to the market.\n",
      "At the market she saw lots of vegetables and fruits. The little girl was not interested in the vegetables and fruits, but her eyes went toward the fish.\n",
      "The little girl said, “I see fish at the market. What kind of fish are they?”\n",
      "The little girl said, “Are they good for my bones?”\n",
      "She said, “Yes, they are, because they are very rich in calcium.”\n",
      "The little girl said, “I would like to eat the fish, but I am not allowed to by my mother.” The little girl saw the owner of the fish. The owner of the fish looked sad and angry. “I would love to sell you some fish, but I can’t do it.”\n",
      "The little girl said, “Why?”\n",
      "The little girl said, “You must be the owner\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 250\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        max_length=max_seq_len,\n",
    "        temperature=1,\n",
    "    )\n",
    "        \n",
    "result = tokenizer.decode(generation_output[0])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neuron_pair_importance_product(gate_weight, up_weight):\n",
    "\n",
    "    gate_norms = torch.norm(gate_weight, p=1, dim=1)\n",
    "    up_norms = torch.norm(up_weight, p=1, dim=1)\n",
    "    importance_scores = gate_norms * up_norms\n",
    "    return importance_scores\n",
    "\n",
    "\n",
    "def compute_neuron_pair_importance_variance(gate_weight, up_weight):\n",
    "    gate_variance = torch.var(gate_weight, dim=1)\n",
    "    up_variance = torch.var(up_weight, dim=1)\n",
    "    importance_scores = gate_variance + up_variance\n",
    "    return importance_scores\n",
    "\n",
    "\n",
    "def compute_neuron_pair_importance_absolute(gate_weight, up_weight):\n",
    "  gate_max_abs = torch.max(gate_weight, dim=1).values + torch.abs(torch.min(gate_weight, dim=1).values)\n",
    "  up_max_abs = torch.max(up_weight, dim=1).values + torch.abs(torch.min(up_weight, dim=1).values)\n",
    "  importance_scores = gate_max_abs + up_max_abs\n",
    "  return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_neuron_pairs(mlp, prune_percent, type):\n",
    "    assert type in ['product', 'variance', 'absolute']\n",
    "    gate_weight = mlp.gate_proj.weight.data.float()\n",
    "    up_weight = mlp.up_proj.weight.data.float()\n",
    "\n",
    "   \n",
    "    if type == 'product':\n",
    "        importance_scores = compute_neuron_pair_importance_product(gate_weight, up_weight)\n",
    "    elif type == 'variance':\n",
    "        importance_scores = compute_neuron_pair_importance_variance(gate_weight, up_weight)\n",
    "    elif type == 'absolute':\n",
    "        importance_scores = compute_neuron_pair_importance_absolute(gate_weight, up_weight)\n",
    "\n",
    "    original_intermediate_size = gate_weight.size(0)\n",
    "\n",
    "    num_neuron_pairs_to_prune = min(int(prune_percent * original_intermediate_size), original_intermediate_size - 1)\n",
    "\n",
    "    k = original_intermediate_size - num_neuron_pairs_to_prune\n",
    "\n",
    "    if k <= 0:\n",
    "        raise ValueError(f\"Invalid number of neuron pairs to keep: {k}. Adjust the prune_percent.\")\n",
    "\n",
    "    _, indices_to_keep = torch.topk(importance_scores, k, largest=True, sorted=True)\n",
    "    indices_to_keep = indices_to_keep.sort().values\n",
    "\n",
    "    #create the new layers\n",
    "    new_gate_proj = nn.Linear(mlp.gate_proj.in_features, k, bias=False).to(device)\n",
    "    new_up_proj = nn.Linear(mlp.up_proj.in_features, k, bias=False).to(device)\n",
    "    new_down_proj = nn.Linear(k, mlp.down_proj.out_features, bias=False).to(device)\n",
    "\n",
    "    #copy weights to the new layers.\n",
    "    new_gate_proj.weight.data = mlp.gate_proj.weight.data[indices_to_keep, :]\n",
    "    new_up_proj.weight.data = mlp.up_proj.weight.data[indices_to_keep, :]\n",
    "    new_down_proj.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
    "\n",
    "    #return new layers and intermediate size.\n",
    "    return new_gate_proj, new_up_proj, new_down_proj, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model, prune_percent, type):\n",
    "    assert type in ['product', 'variance', 'absolute']\n",
    "    new_intermediate_size = None\n",
    "\n",
    "    #loop for each model layer.\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        #Since each layer is a LlamaDecoderLayer it contains multiple components\n",
    "        # Attention, MLP and Layer norms. We're targetting MLP component\n",
    "        # by accesing layer.mlp.\n",
    "        mlp = layer.mlp\n",
    "\n",
    "        #Call the prune_neiron_pairs with the layers and receiving the pruned.\n",
    "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(mlp, prune_percent, type)\n",
    "\n",
    "        #Replace the Origiginal Layers with Pruned Layers.\n",
    "        mlp.gate_proj = new_gate_proj\n",
    "        mlp.up_proj = new_up_proj\n",
    "        mlp.down_proj = new_down_proj\n",
    "\n",
    "        #new_intermediate_size only needs to be set once\n",
    "        if new_intermediate_size is None:\n",
    "            new_intermediate_size = new_size\n",
    "\n",
    "    #Update the model config file.\n",
    "    model.config.intermediate_size = new_intermediate_size\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_percent = 0.2  # Prune 20% of neurons\n",
    "\n",
    "# ppl_model = PPLMetric(model, tokenizer, ['wikitext2'], device)\n",
    "\n",
    "model_absolute = update_model(model, prune_percent, 'absolute')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.model = model_absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model parameters: 1074792448\n",
      "Reduction in parameters: 161021952\n",
      "Percentage of weight savings: 13.03%\n"
     ]
    }
   ],
   "source": [
    "pruned_param_count = count_parameters(model_config.model)\n",
    "reduction_in_params = original_param_count - pruned_param_count\n",
    "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
    "\n",
    "print(f\"Pruned model parameters: {pruned_param_count}\")\n",
    "print(f\"Reduction in parameters: {reduction_in_params}\")\n",
    "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2458791 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference Time: 0.0245 seconds\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'seqlen'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainloader , testloader \u001b[38;5;241m=\u001b[39m get_wikitext2(model_config\u001b[38;5;241m.\u001b[39mnsamples, model_config\u001b[38;5;241m.\u001b[39mseed, model_config\u001b[38;5;241m.\u001b[39mseqlen, model_config\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mglobal_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m30\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_structured\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLP/project/pruning-llm-nlp-project/src/evaluation_pruning.py:251\u001b[0m, in \u001b[0;36mglobal_evaluation\u001b[0;34m(modelConfig, ratio, trainloader, testloader, is_structured, device)\u001b[0m\n\u001b[1;32m    247\u001b[0m     inference_time \u001b[38;5;241m=\u001b[39m measure_inference_time(modelConfig\u001b[38;5;241m.\u001b[39mmodel, modelConfig\u001b[38;5;241m.\u001b[39mnsamples, modelConfig\u001b[38;5;241m.\u001b[39mseed, \n\u001b[1;32m    248\u001b[0m                                             modelConfig\u001b[38;5;241m.\u001b[39mseqlen, modelConfig\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Perplexity evaluation\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m ppl_test \u001b[38;5;241m=\u001b[39m \u001b[43meval_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Memory evaluation\u001b[39;00m\n\u001b[1;32m    254\u001b[0m model_size_in_Mo \u001b[38;5;241m=\u001b[39m get_model_size(modelConfig\u001b[38;5;241m.\u001b[39mmodel, ratio)\n",
      "File \u001b[0;32m~/NLP/project/pruning-llm-nlp-project/src/evaluation_pruning.py:89\u001b[0m, in \u001b[0;36meval_perplexity\u001b[0;34m(model, testloader, device)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_perplexity\u001b[39m(model, testloader, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Evaluate ppl in no grad context to avoid updating the model\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 89\u001b[0m         ppl_test \u001b[38;5;241m=\u001b[39m \u001b[43meval_ppl_wikitext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ppl_test\n",
      "File \u001b[0;32m~/NLP/project/pruning-llm-nlp-project/src/evaluation_pruning.py:44\u001b[0m, in \u001b[0;36meval_ppl_wikitext\u001b[0;34m(model, testenc, bs, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m testenc \u001b[38;5;241m=\u001b[39m testenc\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Calculate number of samples\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m nsamples \u001b[38;5;241m=\u001b[39m testenc\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseqlen\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# List to store negative log likelihoods\u001b[39;00m\n\u001b[1;32m     47\u001b[0m nlls \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'seqlen'"
     ]
    }
   ],
   "source": [
    "trainloader , testloader = get_wikitext2(model_config.nsamples, model_config.seed, model_config.seqlen, model_config.tokenizer)\n",
    "\n",
    "global_evaluation(model_config, ratio='30', trainloader=trainloader, testloader=testloader, is_structured=True, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Tell me a storie. Once upon a time, there was a little boy and his mother who owned a chicken egg business. The child wanted to help, but didn't even know how to get into a tractor box. So he asked the mother how to open the box.\n",
      "The mother told him to keep the finger under her lip and punch the door.\n",
      "Oh, oh, oh!\n",
      "In the end, the boy helped the mother in removing the chicken egg and filling it with egg shells.\n",
      "The child returned to the truck box.\n",
      "Oh, oh, ow!\n",
      "The father turned and said \"How did you do that?\"\n",
      "The child told him he simply pushed his thumb over the door.\n",
      "He didn't even know how to manage a little thing, but he did get the egg out of the container.\n",
      "The mother thanked him for his help.\n",
      "What's wrong with this boy? You shouldn't have to work at all! Just let the kid do whatever he likes.\n",
      "My mother was the chicken egg woman\n",
      "When she asked me, how do you open a chicken egg box?\n",
      "It's okay, my dear,\n",
      "Tell me a storie, if you want to be a happy chicken!\n",
      "Once I was going to work for you, you wouldn\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "\n",
    "    generation_output = model_absolute.generate(\n",
    "        input_ids=input_ids,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        max_length=max_seq_len,\n",
    "        temperature=1,\n",
    "    )\n",
    "        \n",
    "result = tokenizer.decode(generation_output[0])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "global_evaluation(model_config, ,  ['wikitext2'], 1, 1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
